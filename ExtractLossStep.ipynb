{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "current_file_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "for i in glob.glob(os.path.join(current_file_dir,\"logs\",\"*.txt\")):\n",
    "    print(i)\n",
    "    baselogfile = os.path.basename(i)\n",
    "    print(baselogfile)\n",
    "    basefilename = os.path.splitext(baselogfile)[0]\n",
    "    output_csv_losslog_name = basefilename+\"_loss.csv\" \n",
    "    output_csv_mAPlog_name = basefilename+\"_mAP.csv\"\n",
    "    print(output_csv_mAPlog_name)\n",
    "    output_csv_log_dir = os.path.join(current_file_dir,\"csvlog\")\n",
    "    if not os.path.exists(output_csv_log_dir):\n",
    "        os.makedirs(output_csv_log_dir)\n",
    "\n",
    "    with open(i, 'rb') as file_in:\n",
    "        #Read logs and filter out the terms\n",
    "        line = list(filter(lambda line: (b'loss =' in line and b'step =' in line), file_in))\n",
    "        loss_step_list=[line[i].decode(\"utf-8\").split(\":\")[-1] for i in range(0,len(line),2)] \n",
    "        #print(loss_step_list)\n",
    "        mAP_Dict = {}\n",
    "        step_loss={}\n",
    "        for i in loss_step_list:\n",
    "            if 'global_step =' in i:\n",
    "                #Extract mAP values and steps\n",
    "                #print(\"Global step section\")\n",
    "                mAP = i.split(\",\")\n",
    "                #print(i)\n",
    "                mAP_value = mAP[0].split(\" = \")[1]\n",
    "                mAP_step = mAP[-3].split(\" = \")[1]\n",
    "                mAP_Dict[str(mAP_step)]=mAP_value\n",
    "            else:\n",
    "                loss_step = i.split(\"(\")[0]\n",
    "                #Strip the string of loss and steps\n",
    "                loss,step = loss_step.split(\",\")[0].strip(),loss_step.split(\",\")[1].strip()\n",
    "                loss_stripped = loss.split(\" = \")[-1]\n",
    "                step_stripped = step.split(\" = \")[-1]\n",
    "                step_loss[str(step_stripped)]=loss_stripped\n",
    "        print(mAP_Dict)\n",
    "        #print(\" \")\n",
    "        print(step_loss)\n",
    "        #Writes log into csv log directory\n",
    "        loss_log_file = os.path.join(output_csv_log_dir,output_csv_losslog_name)\n",
    "        with open(loss_log_file, \"w+\") as f1:  # Just use 'w' mode in 3.x\n",
    "            w = csv.writer(f1)\n",
    "            w.writerow([\"steps\",\"losses\"])\n",
    "            w.writerows(step_loss.items())\n",
    "        mAP_log_file = os.path.join(output_csv_log_dir,o\n",
    "                                    utput_csv_mAPlog_name)\n",
    "        with open(mAP_log_file,\"w+\") as f2:\n",
    "            w = csv.writer(f2)\n",
    "            w.writerow([\"steps\",\"Average_Precision\"])\n",
    "            w.writerows(mAP_Dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qzq/Documents/Dockers/tf_objectdetection/Vedai/logs/output_rfcn_resnet_variant_50k.log\n",
      "output_rfcn_resnet_variant_50k.log\n",
      "Loss step list: ['INFO - loss = 3.680578, step = 0\\n', 'INFO - loss = 2.668031, step = 100 (75.389 sec)\\n', 'INFO - Saving dict for global step 200: DetectionBoxes_Precision/mAP = 0.0, DetectionBoxes_Precision/mAP (large) = 0.0, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.0, DetectionBoxes_Precision/mAP@.75IOU = 0.0, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0, DetectionBoxes_Recall/AR@100 = 0.0, DetectionBoxes_Recall/AR@100 (large) = 0.0, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/BoxClassifierLoss/classification_loss = 1.5329058, Loss/BoxClassifierLoss/localization_loss = 0.0005072113, Loss/RPNLoss/localization_loss = 0.20861152, Loss/RPNLoss/objectness_loss = 0.70668024, Loss/total_loss = 2.4487047, global_step = 200, learning_rate = 1e-06, loss = 2.4487047\\n', 'INFO - loss = 2.3617282, step = 200 (109.147 sec)\\n', 'INFO - loss = 1.7892604, step = 300 (70.636 sec)\\n', 'INFO - Saving dict for global step 350: DetectionBoxes_Precision/mAP = 0.0, DetectionBoxes_Precision/mAP (large) = 0.0, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.0, DetectionBoxes_Precision/mAP@.75IOU = 0.0, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0, DetectionBoxes_Recall/AR@100 = 0.0, DetectionBoxes_Recall/AR@100 (large) = 0.0, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/BoxClassifierLoss/classification_loss = 0.93656605, Loss/BoxClassifierLoss/localization_loss = 0.00051983434, Loss/RPNLoss/localization_loss = 0.2083525, Loss/RPNLoss/objectness_loss = 0.6956745, Loss/total_loss = 1.8411125, global_step = 350, learning_rate = 1e-06, loss = 1.8411125\\n', 'INFO - Loss for final step: 1.4437281.\\n']\n",
      "08-02-2020 22:05:02\n",
      "08-02-2020 22:11:39\n",
      "397.0\n",
      "mAP_value : 0.0\n",
      "mAP_step : 200\n",
      "mAP_value : 0.0\n",
      "mAP_step : 350\n",
      "Loss_content: INFO - loss = 3.680578, step = 0\n",
      "\n",
      "Loss: 3.680578\n",
      "Step: 0\n",
      "Loss_content: INFO - loss = 2.668031, step = 100 \n",
      "Loss: 2.668031\n",
      "Step: 100\n",
      "Loss_content: INFO - loss = 2.3617282, step = 200 \n",
      "Loss: 2.3617282\n",
      "Step: 200\n",
      "Loss_content: INFO - loss = 1.7892604, step = 300 \n",
      "Loss: 1.7892604\n",
      "Step: 300\n",
      "mAP_list: ['INFO - Saving dict for global step 200: DetectionBoxes_Precision/mAP = 0.0, DetectionBoxes_Precision/mAP (large) = 0.0, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.0, DetectionBoxes_Precision/mAP@.75IOU = 0.0, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0, DetectionBoxes_Recall/AR@100 = 0.0, DetectionBoxes_Recall/AR@100 (large) = 0.0, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/BoxClassifierLoss/classification_loss = 1.5329058, Loss/BoxClassifierLoss/localization_loss = 0.0005072113, Loss/RPNLoss/localization_loss = 0.20861152, Loss/RPNLoss/objectness_loss = 0.70668024, Loss/total_loss = 2.4487047, global_step = 200, learning_rate = 1e-06, loss = 2.4487047\\n', 'INFO - Saving dict for global step 350: DetectionBoxes_Precision/mAP = 0.0, DetectionBoxes_Precision/mAP (large) = 0.0, DetectionBoxes_Precision/mAP (medium) = 0.0, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.0, DetectionBoxes_Precision/mAP@.75IOU = 0.0, DetectionBoxes_Recall/AR@1 = 0.0, DetectionBoxes_Recall/AR@10 = 0.0, DetectionBoxes_Recall/AR@100 = 0.0, DetectionBoxes_Recall/AR@100 (large) = 0.0, DetectionBoxes_Recall/AR@100 (medium) = 0.0, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/BoxClassifierLoss/classification_loss = 0.93656605, Loss/BoxClassifierLoss/localization_loss = 0.00051983434, Loss/RPNLoss/localization_loss = 0.2083525, Loss/RPNLoss/objectness_loss = 0.6956745, Loss/total_loss = 1.8411125, global_step = 350, learning_rate = 1e-06, loss = 1.8411125\\n']\n",
      "{'200': '0.0', '350': '0.0'}\n",
      "Step loss: {'0': '3.680578', '100': '2.668031', '200': '2.3617282', '300': '1.7892604', '350': '1.4437281.'}\n",
      "{'Started': '08-02-2020 22:05:02', 'Ended': '08-02-2020 22:11:39', 'Duration': '397.0'}\n",
      "dict_values(['08-02-2020 22:05:02', '08-02-2020 22:11:39', '397.0'])\n"
     ]
    }
   ],
   "source": [
    "#Updated to suit changes in model_main.py with added time,start,end\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "current_file_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "for i in glob.glob(os.path.join(current_file_dir,\"logs\",\"*.log\")):\n",
    "    print(i)\n",
    "    baselogfile = os.path.basename(i)\n",
    "    print(baselogfile)\n",
    "    basefilename = os.path.splitext(baselogfile)[0]\n",
    "    output_csv_losslog_name = basefilename+\"_loss.csv\" \n",
    "    output_csv_mAPlog_name = basefilename+\"_mAP.csv\"\n",
    "    output_csv_durationlog_name = basefilename+\"_duration.csv\"\n",
    "    output_csv_log_dir = os.path.join(current_file_dir,\"csvlog\")\n",
    "    #Make directory if it does not exists\n",
    "    if not os.path.exists(output_csv_log_dir):\n",
    "        os.makedirs(output_csv_log_dir)\n",
    "\n",
    "    with open(i, 'r') as file_in:\n",
    "        #Read logs and filter out the terms\n",
    "        list_format = list(file_in)\n",
    "        #print(list_format)\n",
    "        loss_step_list = [i for i in list_format if (' loss = ' in i and ' step = ' in i) or 'Saving dict for global step' in i or ' Loss for final step:' in i]\n",
    "        print(\"Loss step list: {}\".format(loss_step_list)) \n",
    "        time_start_end_info = [i for i in list_format if ' Started:' in i or ' Ended:' in i or ' Duration:' in i]\n",
    "        #print(time_start_end_info)\n",
    "        mAP_list = [i for i in list_format if 'Saving dict for global step ' in i ]\n",
    "        #print(mAP_list)\n",
    "\n",
    "        mAP_dict = {}\n",
    "        step_loss={}\n",
    "        time_dict = {}\n",
    "        for i in time_start_end_info:\n",
    "            if \"Started:\" in i:\n",
    "                start_time = i.split(\":\",1)[1].strip()\n",
    "                print(start_time)\n",
    "                time_dict[\"Started\"]= str(start_time)\n",
    "            if \"Ended:\" in i:\n",
    "                end_time = i.split(\":\",1)[1].strip()\n",
    "                print(end_time)\n",
    "                time_dict[\"Ended\"]= str(end_time)\n",
    "            if \"Duration:\" in i:\n",
    "                duration = i.split(\":\",1)[1].strip().split(\" \")[0]\n",
    "                print(duration)\n",
    "                time_dict[\"Duration\"] = str(duration)\n",
    "\n",
    "        for i in mAP_list:\n",
    "            if 'global_step =' in i:\n",
    "                #Extract mAP values and steps for each entry\n",
    "                #print(\"Global step section\")\n",
    "                mAP = i.split(\",\")\n",
    "                #print(mAP)\n",
    "                mAP_value = mAP[0].split(\" = \")[1]\n",
    "                print(\"mAP_value : {}\".format(mAP_value))\n",
    "                mAP_step = mAP[-3].split(\" = \")[1]\n",
    "                print(\"mAP_step : {}\".format(mAP_step))\n",
    "                mAP_dict[str(mAP_step)]=mAP_value\n",
    "                \n",
    "        for i in loss_step_list:\n",
    "            \n",
    "            if \"INFO - loss =\" in i and \"step =\" in i:\n",
    "                loss_step_content = i.split(\"(\")[0]\n",
    "                print(\"Loss_content: {}\".format(loss_step_content))\n",
    "                loss,step = loss_step_content.split(\",\")[0].split(\"=\")[1].strip(),loss_step_content.split(\",\")[1].split(\"=\")[1].strip()\n",
    "                print(\"Loss: {}\".format(loss))\n",
    "                print(\"Step: {}\".format(step))\n",
    "                step_loss[step]= loss\n",
    "            if \"Loss for final step:\" in i:\n",
    "                last_step_value= str(max(mAP_dict.keys()))\n",
    "                final_loss= i.split(\":\")[1].strip()\n",
    "                #print(final_loss)\n",
    "                print(\"mAP_list: {}\".format(mAP_list))\n",
    "                step_loss[last_step_value] = final_loss\n",
    "        print(mAP_dict)\n",
    "        print(\"Step loss: {}\".format(step_loss))\n",
    "        print(time_dict)\n",
    " \n",
    "        #Writes log into csv log directory\n",
    "        \n",
    "        loss_log_file = os.path.join(output_csv_log_dir,output_csv_losslog_name)\n",
    "        with open(loss_log_file, \"w+\") as f1:  # Just use 'w' mode in 3.x\n",
    "            w = csv.writer(f1)\n",
    "            w.writerow([\"steps\",\"losses\"])\n",
    "            w.writerows(step_loss.items())\n",
    "        mAP_log_file = os.path.join(output_csv_log_dir,output_csv_mAPlog_name)\n",
    "        with open(mAP_log_file,\"w+\") as f2:\n",
    "            w = csv.writer(f2)\n",
    "            w.writerow([\"steps\",\"Average_Precision\"])\n",
    "            w.writerows(mAP_dict.items())\n",
    "        durationlog_file = os.path.join(output_csv_log_dir,output_csv_durationlog_name)\n",
    "        with open(durationlog_file,\"w+\") as f3:\n",
    "            w = csv.writer(f3)\n",
    "            w.writerow([\"Start\",\"End\",\"Duration(seconds)\"])\n",
    "            print(time_dict.values())\n",
    "            w.writerow([time_dict[\"Started\"],time_dict[\"Ended\"],time_dict[\"Duration\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfobjectdetection] *",
   "language": "python",
   "name": "conda-env-tfobjectdetection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
